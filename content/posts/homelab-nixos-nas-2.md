+++
title = "My NixOS NASğŸ§Š (2)"
date = 2025-10-17
+++

# My NixOS NASğŸ§Š (2)

2025-10-17

You can find [my configuration here](https://github.com/antipeth/atplab).
You can find [my previous article here](https://blog.0pt.icu/posts/homelab-nixos-nas/).

---

## ğŸª„ Introduction

Back in **March**, I built my NAS using **NixOS**, and itâ€™s been running **rock-solid** ever since ğŸ’ª.
However, this August I made some big hardware and backup changes. For some reason, I bought an extra **32 GB RAM stick**, bringing my NASâ€™s total memory to **40 GB** ğŸ§ .

I also purchased a **Western Digital HC620 zoned mechanical drive (SMR)** â€” **14 TB** â€” to store my static media files ğŸ¬.
I couldnâ€™t format this drive using `nixos disko`, but `mkfs` worked perfectly, and mounting it with `filesystem` caused no conflicts at all.

| Name        | Content                |
| ----------- | ---------------------- |
| Motherboard | GIGABYTE A520i Dash    |
| CPU         | R5 5600G               |
| Memory      | 8 GB + 32 GB DDR4      |
| SSD         | 512 GB NVMe PCIe 3 Ã— 4 |
| HDD         | 2 Ã— 1 TB               |
| HDD         | 14 TB                  |

Because my **dorm cuts power every night** ğŸ”Œ, I canâ€™t keep the mechanical drives constantly connected â€” it would wear them out too fast.
So I turned them into **cold backups**, doing a scheduled sync **once per month** ğŸ§Š.
Of course, my local data is still **backed up to S3 daily** â˜ï¸.

I also bought a **VPS** to host my online services ğŸŒ.

So, it was time to **rebuild my NixOS NAS** â€” transforming it into a proper **NixOS-based homelab environment** ğŸ§°.

---

## ğŸ—ï¸ Rebuild

### ğŸ” Replacing `clan` with `nixos-rebuild`

In my previous article, I highly recommended the `clan` tool.
Unfortunatelyâ€¦ Iâ€™ve decided to drop it ğŸ˜….

1. Since March, `clan` has had major updates â€” mostly around the `inventory` system. But I donâ€™t really need that; I want my setup to stay **as close to pure NixOS** as possible.
2. One big reason I originally used `clan` was for **multi-host deployment** and management. But after an update, I found that when deploying just one machine, `clan` still **checks the configuration of all machines** â€” which conflicts with my **SOPS-encrypted values and secrets** approach ğŸ”.
3. Also, even though I configure my network through `networking` and _not_ `systemd-network`, `clan` still automatically enables it ğŸ¤”. This isnâ€™t a big issue locally or on some VPSs, but on certain ones it **completely breaks IPv6** ğŸ’¥.

`clan` is a great project â€” just not the right fit for me anymore.
So I switched back to the good old **`nixos-rebuild`** ğŸ§±.

For multi-machine deployment, I now use a **Justfile** with scripts that dynamically insert the hostname using `seq`, and I simply **avoid committing these transient changes** to Git ğŸ§¹.

---

### ğŸ”’ Using `nixos-sops`

Previously, I used the SOPS integration built into `clan`.
Now I use **SOPS directly**, with a cleaner directory structure for encryption ğŸ”§.

Letâ€™s look at what changed ğŸ‘‡

**Before (with `clan`):**

- Secrets were stored in `sops/secrets/`.
- Encrypted values or strings were stored in `sops/eval/<machine-name>/`.

**After the rebuild:**

- Secrets are now stored in `machines/<machine-name>/secrets/`.
- Encrypted values or strings are now stored in `machines/<machine-name>/values/`.

You can find my related article about SOPS encryption here. ğŸ‘‡

- [Sops-nix Quick Start Guide](https://blog.0pt.icu/posts/nixos-sops-nix-quick-start-guide/)
- [Using SOPS to Encrypt Nix Values](https://blog.0pt.icu/posts/nixos-using-sops-to-encrypt-nix-values/)

This makes import paths in Nix much simpler âœ¨.

---

### ğŸ” Switching from ACME Certificates to Self-Signed Certificates

At first, my NixOS NAS used **ACME** certificates, integrated nicely with Nginx.
But once I added a VPS, I needed a way to **distribute certificates** among machines ğŸ“¨.

Here was my initial idea ğŸ’­:

1. The VPS runs a **Headscale** server, and both cloud and local machines join the Headscale network ğŸ•¸ï¸.
2. The VPS obtains certificates via **ACME**.
3. The VPS uses **rclone** to encrypt and store the certs in another local directory.
4. Nginx on the VPS serves an **HTTPS static file server** (inside the Headscale network, using the domain generated by `magic_dns`) ğŸŒ.
5. Other machines use **rcloneâ€™s HTTP backend** to periodically pull and decrypt the certs locally ğŸ”.

But when I actually started setting it upâ€¦ I got lazy ğŸ˜†.
So I just went with **self-signed certificates** â€” zero maintenance required.

For local services, thatâ€™s perfectly fine âœ….
For cloud services, combining **self-signed certs + CDN** is totally enough for a personal **homelab** setup ğŸ .

---

### ğŸ§© Other Improvements

- â˜ï¸ Added Headscale in the cloud to manage all devices within a private network.
- ğŸ—‚ï¸ Reorganized the **directory structure**.
- ğŸŒ€ Used a **recursive import function** (`lib.filesystem.listFilesRecursive`) to simplify Nix code.
- ğŸ“ All example service modules are now stored under `machines/example/modules/`.
